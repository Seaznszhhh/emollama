from langchain_community.llms import Ollama
import streamlit as st
from IPython.display import HTML, display
from io import BytesIO
from PIL import Image
import numpy as np
import base64
from generate_audio_tool import audiotool
import os

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
#from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
#from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

def page2(c,conn,user_name):
    def convert_to_base64(pil_image):
        """
        Convert PIL images to Base64 encoded strings

        """
        buffered = BytesIO()
        pil_image.save(buffered, format="JPEG")  # You can change the format if needed
        img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
        return img_str

    def plt_img_base64(img_base64):
        """
        Display base64 encoded string as image
        """
        image_html = f'<img src="data:image/jpeg;base64,{img_base64}" />'
        display(HTML(image_html))

    def generate_img_caption(img_file_buffer):
            print("--------------------å¼€å§‹ç”Ÿæˆå›¾åƒç†è§£---------------------")
            img = Image.open(img_file_buffer)
            img_array = np.array(img)
            st.image(img)
            pil_image = Image.fromarray(img_array)
            image_b64 = convert_to_base64(pil_image)
            plt_img_base64(image_b64)
            llm_with_image_context = llm.bind(images=[image_b64])
            return llm_with_image_context.invoke("ç”¨ç®€çŸ­çš„è¯­è¨€æè¿°å›¾ç‰‡ä¸­äººç‰©çš„å¿ƒæƒ…")
        
    st.markdown("# ğŸ¦œğŸ”—å¿ƒç†å¥åº·æ²»ç–—æœºå™¨äºº")
    model_list = ["qwen:14b-chat", "llava:7b"]
    object_list = ["å­•æœŸ","ä¸­å°å­¦ç”Ÿ","è€å¹´äºº"]

    if "messages" not in st.session_state:
        st.session_state.messages = []

    st.title(f"ğŸ¤©Emollama")
    with st.sidebar:
        if st.button("è¿”å›"):
            st.session_state['page'] = 'page1'
            st.experimental_rerun()
        if st.button("æŸ¥çœ‹å­˜æ¡£"):
            st.session_state['page'] = 'page3'
            st.experimental_rerun()
        if st.button("ğŸ§¸å¿ƒç†å¥åº·è¯Šæ–­"):
            st.session_state['page'] = 'page4'
            st.experimental_rerun()
        st.subheader("ğŸš€Settings")

        #é€‰æ‹©å¤§æ¨¡å‹
        llm_option = st.selectbox(
            'é€‰æ‹©ä½ æƒ³ç”¨çš„å¤§æ¨¡å‹ğŸ§‘ğŸ¼â€ğŸ’»',
            model_list)
        st.write('ä½ é€‰æ‹©äº†:', llm_option)

        #é€‰æ‹©å¯¹è±¡
        object_option = st.selectbox(
        'é€‰æ‹©é€‚ç”¨äººç¾¤ğŸ™‹ğŸ¼â€â™€ï¸',
        object_list)
        st.write('ä½ é€‰æ‹©äº†:', object_option)

        #ä¸Šä¼ å›¾ç‰‡
        img_file_buffer = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", accept_multiple_files=False)
        if img_file_buffer is not None:
            llm = Ollama(model="llava:7b")
            img_describe = generate_img_caption(img_file_buffer=img_file_buffer)
            st.markdown(img_describe)

        #è¿æ¥æ‘„åƒå¤´
        picture = st.camera_input("Take a picture")
        if picture is not None:
            llm = Ollama(model="llava:7b")
            img_describe = generate_img_caption(img_file_buffer=picture)
            st.markdown(img_describe)

    class RAG:
        def __init__(self, img_describe, llm, age="å­•æœŸ", ):
            self.llm = llm
            self.img_describe=img_describe
            self.age = age
            embeddings = OllamaEmbeddings()
            #def load_text(self):
            if self.age =="ä¸­å°å­¦ç”Ÿ":
                loader = TextLoader("./knowledge/01student.txt", encoding = "utf-8")
                persist_directory = './data_base/vector_db_for_student/chroma'  
            elif self.age =="å­•æœŸ":
                loader = TextLoader("./knowledge/02pregnent.txt", encoding = "utf-8")
                persist_directory = './data_base/vector_db_for_pregent/chroma'  
            elif self.age =="è€å¹´äºº":
                loader = TextLoader("./knowledge/03aged.txt", encoding = "utf-8") 
                persist_directory = './data_base/vector_db_for_aged/chroma'  
            #æ²¡æœ‰æ•°æ®åº“æ—¶æ‰è¿›è¡Œ æé«˜è¿è¡Œæ•ˆç‡  
            if not os.path.exists(persist_directory):
                print("--------------------æ­£åœ¨ç”Ÿæˆæ–°æ•°æ®åº“---------------------")
                documents = loader.load()
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
                split_docs = text_splitter.split_documents(documents)
                self.db = Chroma.from_documents(documents=split_docs,embedding=embeddings,persist_directory=persist_directory)
                # å°†åŠ è½½çš„å‘é‡æ•°æ®åº“æŒä¹…åŒ–åˆ°ç£ç›˜ä¸Š
                self.db.persist()
            #æœ‰æ•°æ®åº“ç›´æ¥è½½å…¥
            else:
                # åŠ è½½æ•°æ®åº“
                print("--------------------æ­£åœ¨è½½å…¥å·²æœ‰æ•°æ®åº“---------------------")
                self.db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)

        def QA(self,query,history=[]):
            print("--------------------å¼€å§‹æ£€ç´¢ç”Ÿæˆ---------------------")
            qa = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type='stuff',
                retriever=self.db.as_retriever(),
                verbose=True)
            prompt = f"""
            ä½ æ˜¯ä¸€ä¸ª{self.age}å¿ƒç†å¥åº·èŠå¤©æœºå™¨äººï¼Œä½ åœ¨é¢å¯¹ä¸€ä¸ªå¯èƒ½æœ‰å¿ƒç†é—®é¢˜çš„ç—…äººï¼Œè¿™æ˜¯ä»–çš„æƒ…ç»ªä¿¡æ¯{img_describe}ï¼Œå›ç­”ä»–çš„é—®é¢˜{query}ï¼Œè¯­æ°”è¦å¾ˆæ¸©æŸ”å¾ˆæ¸©æŸ”
            """
            answer=(qa(prompt)["result"])
            print(answer)
            return(answer)        

    llm = Ollama(model=llm_option)

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    if prompt := st.chat_input("ğŸ«°è¯´ç‚¹ä»€ä¹ˆå§"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            my_RAG=RAG(llm=llm, age=object_option, img_describe=img_describe )
            response = my_RAG.QA(prompt)
            c.execute("INSERT INTO chats (user_name, message, response) VALUES (?, ?, ?)", (user_name,prompt,response))
            audiotool(response)
            st.audio("./audio/1.mp3", format="audio/mpeg")
            st.markdown(response)
        conn.commit()
        st.session_state.messages.append({"role": "assistant", "content": response})


